28          Matrix Operations




            Because operations on matrices lie at the heart of scientiﬁc computing, efﬁcient al-
            gorithms for working with matrices have many practical applications. This chapter
            focuses on how to multiply matrices and solve sets of simultaneous linear equa-
            tions. Appendix D reviews the basics of matrices.
               Section 28.1 shows how to solve a set of linear equations using LUP decomposi-
            tions. Then, Section 28.2 explores the close relationship between multiplying and
            inverting matrices. Finally, Section 28.3 discusses the important class of symmetric
            positive-deﬁnite matrices and shows how we can use them to ﬁnd a least-squares
            solution to an overdetermined set of linear equations.
               One important issue that arises in practice is numerical stability. Due to the
            limited precision of ﬂoating-point representations in actual computers, round-off
            errors in numerical computations may become ampliﬁed over the course of a com-
            putation, leading to incorrect results; we call such computations numerically un-
            stable. Although we shall brieﬂy consider numerical stability on occasion, we do
            not focus on it in this chapter. We refer you to the excellent book by Golub and
            Van Loan [144] for a thorough discussion of stability issues.


28.1 Solving systems of linear equations

            Numerous applications need to solve sets of simultaneous linear equations. We
            can formulate a linear system as a matrix equation in which each matrix or vector
            element belongs to a ﬁeld, typically the real numbers R. This section discusses how
            to solve a system of linear equations using a method called LUP decomposition.
               We start with a set of linear equations in n unknowns x1 ; x2 ; : : : ; xn :
