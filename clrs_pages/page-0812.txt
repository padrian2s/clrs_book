27.1 The basics of dynamic multithreading                                        791


                0
seconds, and T512  D 1024=512 C 8 D 10 seconds. The optimization that sped up
the program on 32 processors would have made the program twice as slow on 512
processors! The optimized versionâ€™s span of 8, which was not the dominant term in
the running time on 32 processors, became the dominant term on 512 processors,
nullifying the advantage from using more processors.
   The moral of the story is that work and span can provide a better means of
extrapolating performance than can measured running times.

Exercises

27.1-1
Suppose that we spawn P-F IB .n  2/ in line 4 of P-F IB, rather than calling it
as is done in the code. What is the impact on the asymptotic work, span, and
parallelism?

27.1-2
Draw the computation dag that results from executing P-F IB .5/. Assuming that
each strand in the computation takes unit time, what are the work, span, and par-
allelism of the computation? Show how to schedule the dag on 3 processors using
greedy scheduling by labeling each strand with the time step in which it is executed.

27.1-3
Prove that a greedy scheduler achieves the following time bound, which is slightly
stronger than the bound proven in Theorem 27.1:
       T1  T1
TP            C T1 :                                                         (27.5)
          P

27.1-4
Construct a computation dag for which one execution of a greedy scheduler can
take nearly twice the time of another execution of a greedy scheduler on the same
number of processors. Describe how the two executions would proceed.

27.1-5
Professor Karan measures her deterministic multithreaded algorithm on 4, 10,
and 64 processors of an ideal parallel computer using a greedy scheduler. She
claims that the three runs yielded T4 D 80 seconds, T10 D 42 seconds, and
T64 D 10 seconds. Argue that the professor is either lying or incompetent. (Hint:
Use the work law (27.2), the span law (27.3), and inequality (27.5) from Exer-
cise 27.1-3.)
