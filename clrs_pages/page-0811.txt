790   Chapter 27 Multithreaded Algorithms


         As an example of how easy it is to generate code with races, here is a faulty
      implementation of multithreaded matrix-vector multiplication that achieves a span
      of ‚.lg n/ by parallelizing the inner for loop:
      M AT-V EC -W RONG .A; x/
      1 n D A:rows
      2 let y be a new vector of length n
      3 parallel for i D 1 to n
      4      yi D 0
      5 parallel for i D 1 to n
      6      parallel for j D 1 to n
      7           yi D yi C aij xj
      8 return y
      This procedure is, unfortunately, incorrect due to races on updating yi in line 7,
      which executes concurrently for all n values of j . Exercise 27.1-6 asks you to give
      a correct implementation with ‚.lg n/ span.
         A multithreaded algorithm with races can sometimes be correct. As an exam-
      ple, two parallel threads might store the same value into a shared variable, and it
      wouldn’t matter which stored the value ﬁrst. Generally, however, we shall consider
      code with races to be illegal.

      A chess lesson
      We close this section with a true story that occurred during the development of
      the world-class multithreaded chess-playing program ?Socrates [80], although the
      timings below have been simpliﬁed for exposition. The program was prototyped
      on a 32-processor computer but was ultimately to run on a supercomputer with 512
      processors. At one point, the developers incorporated an optimization into the pro-
      gram that reduced its running time on an important benchmark on the 32-processor
                                               0
      machine from T32 D 65 seconds to T32       D 40 seconds. Yet, the developers used
      the work and span performance measures to conclude that the optimized version,
      which was faster on 32 processors, would actually be slower than the original ver-
      sion on 512 processsors. As a result, they abandoned the “optimization.”
         Here is their analysis. The original version of the program had work T1 D 2048
      seconds and span T1 D 1 second. If we treat inequality (27.4) as an equation,
      TP D T1 =P C T1 , and use it as an approximation to the running time on P pro-
      cessors, we see that indeed T32 D 2048=32 C 1 D 65. With the optimization, the
      work became T10 D 1024 seconds and the span became T1        0
                                                                     D 8 seconds. Again
                                           0
      using our approximation, we get T32 D 1024=32 C 8 D 40.
         The relative speeds of the two versions switch when we calculate the running
      times on 512 processors, however. In particular, we have T512 D 2048=512C1 D 5
