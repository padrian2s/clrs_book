5.2 Indicator random variables                                                   119


Proof By the deﬁnition of an indicator random variable from equation (5.1) and
the deﬁnition of expected value, we have
E ŒXA  D E ŒI fAg
                             ˚
        D 1  Pr fAg C 0  Pr A
        D Pr fAg ;
where A denotes S  A, the complement of A.

   Although indicator random variables may seem cumbersome for an application
such as counting the expected number of heads on a ﬂip of a single coin, they are
useful for analyzing situations in which we perform repeated random trials. For
example, indicator random variables give us a simple way to arrive at the result
of equation (C.37). In this equation, we compute the number of heads in n coin
ﬂips by considering separately the probability of obtaining 0 heads, 1 head, 2 heads,
etc. The simpler method proposed in equation (C.38) instead uses indicator random
variables implicitly. Making this argument more explicit, we let Xi be the indicator
random variable associated with the event in which the ith ﬂip comes up heads:
Xi D I fthe ith ﬂip results in the event H g. Let X be the random variable denoting
the total number of heads in the n coin ﬂips, so that
      X
      n
XD           Xi :
      i D1

We wish to compute the expected number of heads, and so we take the expectation
of both sides of the above equation to obtain
           " n      #
             X
E ŒX  D E       Xi :
                i D1

The above equation gives the expectation of the sum of n indicator random vari-
ables. By Lemma 5.1, we can easily compute the expectation of each of the random
variables. By equation (C.21)—linearity of expectation—it is easy to compute the
expectation of the sum: it equals the sum of the expectations of the n random
variables. Linearity of expectation makes the use of indicator random variables a
powerful analytical technique; it applies even when there is dependence among the
random variables. We now can easily compute the expected number of heads:
