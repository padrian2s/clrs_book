28.2 Inverting matrices                                                        831


The second line holds because the second regularity condition in the statement
of the theorem implies that 4M.n=2/ < 2M.n/ and because we assume that
M.n/ D .n2 /. The third line follows because the second regularity condition
allows us to apply case 3 of the master theorem (Theorem 4.1).
   It remains to prove that we can obtain the same asymptotic running time for ma-
trix multiplication as for matrix inversion when A is invertible but not symmetric
and positive-deﬁnite. The basic idea is that for any nonsingular matrix A, the ma-
trix AT A is symmetric (by Exercise D.1-2) and positive-deﬁnite (by Theorem D.6).
The trick, then, is to reduce the problem of inverting A to the problem of invert-
ing AT A.
   The reduction is based on the observation that when A is an n n nonsingular
matrix, we have
A1 D .AT A/1 AT ;
since ..AT A/1 AT /A D .AT A/1 .AT A/ D In and a matrix inverse is unique.
Therefore, we can compute A1 by ﬁrst multiplying AT by A to obtain AT A, then
inverting the symmetric positive-deﬁnite matrix AT A using the above divide-and-
conquer algorithm, and ﬁnally multiplying the result by AT . Each of these three
steps takes O.M.n// time, and thus we can invert any nonsingular matrix with real
entries in O.M.n// time.

   The proof of Theorem 28.2 suggests a means of solving the equation Ax D b
by using LU decomposition without pivoting, so long as A is nonsingular. We
multiply both sides of the equation by AT , yielding .AT A/x D AT b. This trans-
formation doesn’t affect the solution x, since AT is invertible, and so we can fac-
tor the symmetric positive-deﬁnite matrix AT A by computing an LU decomposi-
tion. We then use forward and back substitution to solve for x with the right-hand
side AT b. Although this method is theoretically correct, in practice the procedure
LUP-D ECOMPOSITION works much better. LUP decomposition requires fewer
arithmetic operations by a constant factor, and it has somewhat better numerical
properties.

Exercises

28.2-1
Let M.n/ be the time to multiply two n n matrices, and let S.n/ denote the time
required to square an n n matrix. Show that multiplying and squaring matri-
ces have essentially the same difﬁculty: an M.n/-time matrix-multiplication al-
gorithm implies an O.M.n//-time squaring algorithm, and an S.n/-time squaring
algorithm implies an O.S.n//-time matrix-multiplication algorithm.
