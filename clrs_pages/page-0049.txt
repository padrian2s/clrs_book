28   Chapter 2 Getting Started


        The “average case” is often roughly as bad as the worst case. Suppose that we
         randomly choose n numbers and apply insertion sort. How long does it take to
         determine where in subarray AŒ1 : : j  1 to insert element AŒj ? On average,
         half the elements in AŒ1 : : j  1 are less than AŒj , and half the elements are
         greater. On average, therefore, we check half of the subarray AŒ1 : : j  1, and
         so tj is about j=2. The resulting average-case running time turns out to be a
         quadratic function of the input size, just like the worst-case running time.
        In some particular cases, we shall be interested in the average-case running time
     of an algorithm; we shall see the technique of probabilistic analysis applied to
     various algorithms throughout this book. The scope of average-case analysis is
     limited, because it may not be apparent what constitutes an “average” input for
     a particular problem. Often, we shall assume that all inputs of a given size are
     equally likely. In practice, this assumption may be violated, but we can sometimes
     use a randomized algorithm, which makes random choices, to allow a probabilistic
     analysis and yield an expected running time. We explore randomized algorithms
     more in Chapter 5 and in several other subsequent chapters.

     Order of growth
     We used some simplifying abstractions to ease our analysis of the I NSERTION -
     S ORT procedure. First, we ignored the actual cost of each statement, using the
     constants ci to represent these costs. Then, we observed that even these constants
     give us more detail than we really need: we expressed the worst-case running time
     as an2 C bn C c for some constants a, b, and c that depend on the statement
     costs ci . We thus ignored not only the actual statement costs, but also the abstract
     costs ci .
        We shall now make one more simplifying abstraction: it is the rate of growth,
     or order of growth, of the running time that really interests us. We therefore con-
     sider only the leading term of a formula (e.g., an2 ), since the lower-order terms are
     relatively insigniﬁcant for large values of n. We also ignore the leading term’s con-
     stant coefﬁcient, since constant factors are less signiﬁcant than the rate of growth
     in determining computational efﬁciency for large inputs. For insertion sort, when
     we ignore the lower-order terms and the leading term’s constant coefﬁcient, we are
     left with the factor of n2 from the leading term. We write that insertion sort has a
     worst-case running time of ‚.n2 / (pronounced “theta of n-squared”). We shall use
     ‚-notation informally in this chapter, and we will deﬁne it precisely in Chapter 3.
        We usually consider one algorithm to be more efﬁcient than another if its worst-
     case running time has a lower order of growth. Due to constant factors and lower-
     order terms, an algorithm whose running time has a higher order of growth might
     take less time for small inputs than an algorithm whose running time has a lower
