27.1 The basics of dynamic multithreading                                       783


  The following corollary to Theorem 27.1 shows that a greedy scheduler always
performs well.

Corollary 27.2
The running time TP of any multithreaded computation scheduled by a greedy
scheduler on an ideal parallel computer with P processors is within a factor of 2
of optimal.

Proof Let TP be the running time produced by an optimal scheduler on a machine
with P processors, and let T1 and T1 be the work and span of the computation,
respectively. Since the work and span laws—inequalities (27.2) and (27.3)—give
us TP  max.T1 =P; T1 /, Theorem 27.1 implies that
TP    T1 =P C T1
      2  max.T1 =P; T1 /
      2TP :

   The next corollary shows that, in fact, a greedy scheduler achieves near-perfect
linear speedup on any multithreaded computation as the slackness grows.

Corollary 27.3
Let TP be the running time of a multithreaded computation produced by a greedy
scheduler on an ideal parallel computer with P processors, and let T1 and T1 be
the work and span of the computation, respectively. Then, if P       T1 =T1 , we
have TP  T1 =P , or equivalently, a speedup of approximately P .

Proof If we suppose that P          T1 =T1 , then we also have T1  T1 =P , and
hence Theorem 27.1 gives us TP  T1 =P C T1  T1 =P . Since the work
law (27.2) dictates that TP  T1 =P , we conclude that TP  T1 =P , or equiva-
lently, that the speedup is T1 =TP  P .

   The     symbol denotes “much less,” but how much is “much less”? As a rule
of thumb, a slackness of at least 10—that is, 10 times more parallelism than pro-
cessors—generally sufﬁces to achieve good speedup. Then, the span term in the
greedy bound, inequality (27.4), is less than 10% of the work-per-processor term,
which is good enough for most engineering situations. For example, if a computa-
tion runs on only 10 or 100 processors, it doesn’t make sense to value parallelism
of, say 1,000,000 over parallelism of 10,000, even with the factor of 100 differ-
ence. As Problem 27-2 shows, sometimes by reducing extreme parallelism, we
can obtain algorithms that are better with respect to other concerns and which still
scale up well on reasonable numbers of processors.
